<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://dkdkkim.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://dkdkkim.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-04-15T04:19:15+00:00</updated><id>https://dkdkkim.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Box2Mask, Box-supervised Instance Segmentation via Level-set Evolution</title><link href="https://dkdkkim.github.io/blog/2023/box2box/" rel="alternate" type="text/html" title="Box2Mask, Box-supervised Instance Segmentation via Level-set Evolution"/><published>2023-01-25T00:00:00+00:00</published><updated>2023-01-25T00:00:00+00:00</updated><id>https://dkdkkim.github.io/blog/2023/box2box</id><content type="html" xml:base="https://dkdkkim.github.io/blog/2023/box2box/"><![CDATA[<p>Segmentation 기술은 지속적으로 발전되어 현재의 SOTA 모델의 성능은 상당히 높은 수준에 이르렀으며, 실제 필드에서도 여러가지 적용사례를 보여주고 있다. 하지만 Segmentation task의 한계는 Labeling cost 라고 할 수 있다. Segmentation을 하기 위한 Ground truth mask 는 pixel 단위로 annotation 을 해야하기 때문에 Classification 이나 detection 등에 비해서 많은 Human resource 를 필요로 한다. 이런 한계를 극복하고자 제안한 접근중에 하나가 Box-supervised Instance Segmentation 이다. 직관적으로 알 수 있듯이 Detection 에 사용되는 Bounding Box를 활용해 Instance Segmentation의 기능을 하도록 하는 것으로 일종의 Weakly supervised learning 이라고 할 수 있다. 다른 Task 에 비해서 비교적 많은 연구가 이루어 지지는 않았지만, 흥미로운 주제이기에 현재 기준으로 SOTA model을 달성한 연구에 대해서 알아보고자 한다.</p> <hr/> <h2 id="problem"><strong>Problem</strong></h2> <p>이미 앞서 설명했지만, 대부분의 instance segmentation 접근법들은 supervised learning 방식으로 이루어지고 있으며, 이에 따라서 pixel level로 mask annotation을 하는데에 상당히 큰 cost를 필요로 한다. box-supervised instance segmentation은 상대적으로 단순한 box annotation 만 있으면 되기 때문에 조금씩 연구가 이루어 지고 있다. Paper with code 에서 box-supervised instance segmentation 의 Ranking을 확인해보면 아래와 같이 2019년 이후로 지속적으로 연구가 이루어 지고 있음을 알 수 있다.</p> <div> <center><img src="../../../assets/img/box01.png" alt="box_rank" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Box-supervised Instance Segmentation on COCO test-dev</center> </div> <hr/> <h2 id="approach"><strong>Approach</strong></h2> <p>본 논문에서는 <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/cd3afef9b8b89558cd56638c3631868a-Paper.pdf">SOLOV2</a> 와 Level Set Segmentation을 활용한 Box-instance segmentation을 제안한다. SOLOV2는 full instance mask annotations를 이용하여 Segmentation 하는 <a href="https://arxiv.org/pdf/1912.04488.pdf">SOLO</a>의 개선 모델이다. Level Set Segmentation은 전통적인 Computer vision 에서 이전부터 사용되던 방법중의 하나이다. 전체적인 Architecture는 아래와 같다.</p> <div> <center><img src="../../../assets/img/box02.png" alt="box_arch" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Architecture of BOX2SEG</center> </div> <hr/> <h2 id="method"><strong>Method</strong></h2> <p>본 논문에서 제안하는 방법은 크게 SOLOV2와 Level Set Evolution 으로 구성되어 있는 것을 알 수 있다. 따라서 이 두가지 방법에 대한 이해가 선행되어야 한다.</p> <h3 id="solo">SOLO</h3> <p>SOLOV2 는 SOLO 의 개선된 버전이기 때문에 SOLO를 먼저 설명하려한다. SOLO 는 기존에 접근과는 다른 방식으로 Instance segmentation을 수행한다. 먼저 Image 를 \(S \times S\) 로 분할하여 접근하며 FPN 의 모든 Feature map 에 대해서 Category branch와 Mask branch가 적용된다. 아래 그림이 SOLO에서 제안하는 방식이다.</p> <div> <center><img src="../../../assets/img/box03.png" alt="small_solo" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Concept of SOLO</center> </div> <p>Category Branch 의 output 은 \(S \times S \times C\) 의 shape를 가지고 있으며 \(S \times S\) 그리드의 Class를 나타낸다.</p> <p>Mask Branch 의 output 은 \(H \times W \times S^2\) 으로 \(S^2\) 개의 그리드에 해당하는 segmentation mask 이다. Mask Branch 에서는 각각의 mask 가 그리드에 match 되도록 하기 위하여 x, y 좌표를 normalize 한 값을 concat 하는 CoordConv 를 사용하였다.</p> <p>결과적으로 두 브랜치의 output을 활용하면 각각의 그리드에 해당하는 Segmentation mask와 Category 를 알 수 있게 된다.</p> <h3 id="solo-v2">SOLO V2</h3> <p>하지만 SOLO 에 대한 방법을 보다보면 걱정이 되는 부분이 생길 수 밖에 없을 것이다. 바로 SOLO의 방법은 높은 Computational Cost를 필요로 한다는 점이다. 이런 단점을 극복한 방법이 SOLO V2 이다. SOLO 에서 사용되었던 Category branch와 Mask branch 대신에 kernel branch와 feature branch 가 새롭게 제안되었다.</p> <div> <center><img src="../../../assets/img/box04.png" alt="small_solo" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Concept of SOLO V2</center> </div> <p>feauter branch 의 output \(F\) 는 \(H \times W \times E\) 의 shape를 가진다. 이때 \(E\) 는 N of channel로 보면 된다.</p> <p>kernel branch 가 key idea 라고 할 수 있는데, 여기서는 \(F\) 에 Convolution 연산을 할 filter 의 weight를 학습하는 역할을 하게 된다. Output \(G\) 는 \(S \times S \times D\) 를 이루게 되는데 이는 \(S^2\) 개의 filter weight를 나타내며 각각의 그리드에 해당한다. 예를들어 \(3 \times 3\) 의 conv filter를 적용하려고 한다면 \(D = E \times (3 \times 3)\) 이된다.</p> <p>결과적으로 \(F\) 와 \(G\) 가 convolution 연산을 하게 되면 SOLO 와 마찬가지로 \(H \time W \times S^2\) 의 segmentation map이 출력된다. 앞서 설명한 방법으로 SOLO V2 는 SOLO 보다 효율적이면서 더 높은 성능을 보였다.</p> <h3 id="level-set-segmentation">Level-set segmentation</h3> <p>Level-set segmentation은 level-set function을 활용하여 image 를 segmentation 하는 방식을 말한다. Level-set function을 \(\phi (x)\) 라고 가정하면 \(\{x \mid \phi (x) = c \}\)를 활용하여 아래와 같이 segmentation을 수행할 수 있다.</p> <div> <center><img src="../../../assets/img/box05.png" alt="small_solo" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Level set segmentation(wikipedia)</center> </div> <p>Level-set segmentaton의 좋은 결과를 내기 위해서는 최적의 level-set function을 찾아야 하는데 이때 사용하는 방식이 Energy function을 활용하는 방식이다. Level-set function 에 대한 Energy function을 정의하여 Energy가 최소화 하도록 Level-set function을 반복하여 업데이트 하여 최적화 하는 방식이다.</p> <div> <center><img src="../../../assets/img/box06.png" alt="small_solo" width="60%" height="60%"/></center> </div> <div class="caption"> <center></center> </div> <p>위와 같은 식으로 업데이트 되는데 이때 initial level-set function과 Energy function을 어떤 방식으로 정의하느냐에 따라서 그 결과는 달라진다. 이 Energy function을 정의 하는 방법중의 하나가 <a href="">Active contours without edges</a> 이다.</p> <div> <center><img src="../../../assets/img/box07.png" alt="small_solo" width="100%" height="60%"/></center> </div> <div class="caption"> <center></center> </div> <p>위의 식이 <a href="https://www.lpi.tel.uva.es/muitic/pim/docus/ActiveContoursWithoutEdges.pdf">Active contours without edges</a>에서 제안한 Energy function 이다. 개념적으로 간단히 설명하자면 4개의 term 으로 이루어져 있고 각각 contour의 length, \(\phi \ge 0\) area의 넗이, inside area, outside area를 의미한다. \(c1, c2\) 는 각각 inside 와 outside의 intensity average를 이다.</p> <h3 id="box2seg">BOX2SEG</h3> <p>본 연구에서 제안한 BOX2SEG 에서는 SOLO V2를 통해서 나온 output 을 initial level-set function \({\phi}_0\) 로 가정하고 SOLO V2의 feature branch의 output을 Conv layer를 통과시킨 \(I_{feat}\), 원본 이미지에서 추출된 feature map \(I_{img}\) 를 활용하여 Level-set evolution을 수행한다.</p> <div> <center><img src="../../../assets/img/box08.png" alt="small_solo" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Level set evolution</center> </div> <p>Level set evolution 에서는 아래와 같은 식으로 Energy function을 정의하는데 Active contours without edges 와 유사한 방식임을 알 수 있다.</p> <div> <center><img src="../../../assets/img/box09.png" alt="small_solo" width="60%" height="60%"/></center> </div> <div class="caption"> <center></center> </div> <p>input feature map \(I_f\), \(I_u\) 를 weighted summation 하여 아래와 같이 최종적인 energy function이 정의되고, 앞서 설명한 방식대로 level-set function을 최적화 한다.</p> <div> <center><img src="../../../assets/img/box10.png" alt="small_solo" width="60%" height="60%"/></center> </div> <div class="caption"> <center></center> </div> <p>모델의 학습에 쓰이는 loss function은 \(wL_{cate} + L_{inst}\) 로 category classification loss 와 instance segmentation loss로 이루어져 있으며 \(L_{cate}\) 는 일반적인 cross entropy loss를 사용하고 \(L_(inst)\) 는 앞서 설명한 energy function \(F(x)\) 와 \(F(\phi_0)_{box}\) 로 이루어져 있는데 \(F(\phi_0)_{box}\) 는 아래 식에서 알 수 있듯이 predicted mask 와 box label을 각 axis 에 projection 했을때 차이를 나타낸다.</p> <div> <center><img src="../../../assets/img/box11.png" alt="small_solo" width="60%" height="60%"/></center> </div> <div class="caption"> <center></center> </div> <p>SOLO와 level-set function 까지 함께 설명하자니 내용이 복잡해졌는데, 여기까지가 BOX2SEG의 Method 에 대한 설명이다.</p> <hr/> <h2 id="experimental-result"><strong>Experimental result</strong></h2> <p>BOX2SEG의 성능을 검증하기 위하여 Pascal VOC, COCO, iSAID, LiTS(Liver tumor), ICDAR2019 ReCTS 의 다양한 데이터 셋에서 실험을 진행하였다. 아래는 COCO dataset에서의 결과인데, box-supervised 에서 가장 좋은 성능을 보이는 것을 알 수 있다. 게다가 mask-supervised 와 비교했을 때에도 대부분의 모델보다 좋은 성능을 보이는 것을 알 수 있다.</p> <div> <center><img src="../../../assets/img/box12.png" alt="small_solo" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Experimental result on COCO test-dev</center> </div> <p>아래 정성적인 결과를 보면 general image 뿐만 아니라 medical image, scene text 등에도 꽤나 효과적인 것을 확인할 수 있다.</p> <div> <center><img src="../../../assets/img/box13.png" alt="small_solo" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Qualitative result of BOX2SEG</center> </div> <hr/> <h2 id="comments"><strong>Comments</strong></h2> <p>최근 효과적인 Large model들에 대한 연구들이 좋은 성과를 내기 시작하고 있다. 성공한 연구들을 보면 NLP나 Image classification 처럼 충분한 데이터를 갖춘 분야들임을 알 수 있다. Segmentation이라는 task는 이런 데이터 관점에서 매우 불리한 Task이고 다른 task 처럼 방대한 데이터셋은 언제 구축될 수 있을 지 모른다. 이런 관점에서 box-supervised segmentation과 같은 접근은 적어도 한동안은 연구되어야 할 분야라는 생각이 든다.</p> <hr/> <h2 id="reference">Reference</h2> <ul> <li><a href="https://arxiv.org/pdf/2212.01579v1.pdf">Li, Wentong, et al. “Box2Mask: Box-supervised Instance Segmentation via Level-set Evolution.” arXiv preprint arXiv:2212.01579 (2022).</a></li> <li><a href="https://arxiv.org/abs/1912.04488">Wang, Xinlong, et al. “Solo: Segmenting objects by locations.” Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVIII 16. Springer International Publishing, 2020.</a></li> <li><a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/cd3afef9b8b89558cd56638c3631868a-Paper.pdf">Wang, Xinlong, et al. “Solov2: Dynamic and fast instance segmentation.” Advances in Neural information processing systems 33 (2020): 17721-17732.</a></li> <li><a href="https://www.lpi.tel.uva.es/muitic/pim/docus/ActiveContoursWithoutEdges.pdf">Chan, Tony F., and Luminita A. Vese. “Active Contours Without Edges.” IEEE TRANSACTIONS ON IMAGE PROCESSING 10.2 (2001).</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Tian_BoxInst_High-Performance_Instance_Segmentation_With_Box_Annotations_CVPR_2021_paper.pdf">Tian, Zhi, et al. “Boxinst: High-performance instance segmentation with box annotations.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.</a></li> <li><a href="https://en.wikipedia.org/wiki/Level-set_method">Level set function, wikipedia</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="review"/><summary type="html"><![CDATA[box-supervised instance segmentation takes advantage of the simple box annotation rather than the pixel-wise mask labels, which has recently attracted a lot of research attentions.]]></summary></entry><entry><title type="html">How to Train Vision Transformer Training on Small Datasets</title><link href="https://dkdkkim.github.io/blog/2022/vit-on-small/" rel="alternate" type="text/html" title="How to Train Vision Transformer Training on Small Datasets"/><published>2022-10-21T00:00:00+00:00</published><updated>2022-10-21T00:00:00+00:00</updated><id>https://dkdkkim.github.io/blog/2022/vit-on-small</id><content type="html" xml:base="https://dkdkkim.github.io/blog/2022/vit-on-small/"><![CDATA[<p>앞서 이야기했듯이 Image recognition의 여러가지 분야에서 Vision Transformer는 많은 성과를 이루어냈다. 하지만 여전히 가지고 있는 한계점은 large-scale의 데이터셋에 한정적으로 효과가 있다는 점이다. 따라서 large-scale의 데이터를 확보하기 힘든 분야에서는 이런 문제로 다양한 ViT 모델이 존재함에도 불구하 적용하는데 어려움을 겪고 있다. 이번에 리뷰하는 논문에서는 이런 단점을 보완하고자 한가지 방법을 제안한다.</p> <hr/> <h2 id="problem"><strong>Problem</strong></h2> <p>ViT가 small-scale dataset을 학습하는 데에 어려움을 겪고 있는 이유는 ViT 알고리즘 특성상 locality, inductive bias, hierarchical sturucture 가 부족하기 때문이다. 비슷한 이유로 ViT는 transfer learning을 하기 위해서 large-scale dataset에 pretraining을 필요로 한다. 결과적으로 large-scale dataset이 없는 경우에는 ViT의 성능은 제한된다.</p> <hr/> <h2 id="approach"><strong>Approach</strong></h2> <p>본 논문에서는 small dataset에서 low-resolution view를 활용해서 sef-supervised learning을 하는 방법을 제안한다. 이 방법은 large dataset에서의 pretraining 없이 ViT를 효과적으로 학습할 수 있게 하는 효과적인 initial weights를 제공한다. 이때 self-supervised learning 하는 방식은 <a href="https://arxiv.org/pdf/2104.14294.pdf">DINO</a>에서 제안한 방식과 아주 유사한 것을 확인할 수 있다. 따라서 DINO를 먼저 숙지한다면 Proposed method는 쉽게 이해할 수 있다.</p> <div> <center><img src="../../../assets/img/small01.gif" alt="small_dino" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Concept of DINO</center> </div> <hr/> <h2 id="method"><strong>Method</strong></h2> <p>앞서 말했듯이 본 논문의 핵심은 Self-supervised learning을 통해 효과적인 initial weights를 찾는 것이다. 따라서 제안하는 방법은 크게 Self-supervised learning 하는 과정과 이때의 weight 값을 활용하여 supervised learning 하는 두가지로 나누어 볼 수 있다.</p> <h3 id="self-supervised-view-prediction-as-weight-initialization-scheme">Self-supervised View Prediction as Weight Initialization Scheme</h3> <p>Self-supervised learning 과정을 보면 아래와 같다. DINO와 마찬가지로 동일한 두개의 모델로 구성되어 있으며 각각 student model과 teacher model 이라고 한다.</p> <div> <center><img src="../../../assets/img/small02.png" alt="small_self" width="50%" height="50%"/></center> </div> <div class="caption"> <center>Self-supervised View Prediction as Weight Initialization Scheme</center> </div> <p>학습을 위해 Input image 에서 일종의 augmentation을 통해 local view와 global view 두가지의 image를 생성한다. 두가지의 차이점은 local view \(x_l\)는 원본이미지의 20~50%의 크기의 crop 된 이미지이고 global view \(x_g\)는 원본이미지의 50%크기 이상인 이미지이라는 점이다. \(x_l\)과 \(x_g\)의 크기의 비율은 1:4가 되도록 한다. 모든 이미지는 color jittering, gray scaling, solarization, random hrizontal flip and gausian blur를 포함한 standar augmentation을 적용한다.</p> <p>Student model \(F_s\)에는 모든 image가 DPE(Dynamic Position Embedding)를 통해 position embeding 처리가 된 후 입력된다. 반면에 Teacher model \(F_g\)에는 global view 만 입력되게 된다. 각각의 모델은 모두 ViT이고 ViT 뒤에 MLP head를 통과 하게 된다. MLP head는 3개의 linear layer로 구성되어 있고, single layer 보다 multi layer가 더 좋은 성능을 보인다고 한다.</p> <p>입력된 이미지와 사용된 모델에 따라서 각각 \(F_s(x_l)\), \(F_s(x_l)\), \(F_t(x_g)\) 이 생성된다. 예를 들면, \(F_s(x_l)\) 의 경우는 local view 가 student model을 통해 생성된 출력값이다. student model은 이 출력값들을 활용하여 아래의 cost function을 통해서 update 된다.</p> \[\mathfrak{L}=-\widetilde{F}_{g_t}\cdot \log(\widetilde{F}_{g_s})+\sum_{i=1}^{n}-\widetilde{F}_{g_t}\cdot \log(\widetilde{F}^{(i)}_{l_s})\] <p>teacher model \(F_t\)는 일반적인 teacher-student 학습처럼 아래의 수식대로 두모델의 weight \({\theta}_{s}\), \({\theta}_{t}\) 의 exponential moving average를 이용하여 업데이트 된다.</p> \[{\theta}_{t} \leftarrow \lambda{\theta}_{t}+(1-\lambda{\theta}_{s})\] <h3 id="self-supervised-to-supervised-label-prediction">Self-supervised to Supervised Label Prediction</h3> <p>앞서 Self-supervised learning을 통해서 획득한 weight 값을 활용하여 supervised learningdmf 진행하게 된다. 두가지 모델중 teacher 모델의 weight를 초기값으로 하여 목표가 되는 supervised learning을 진행하며 이 과정은 일반적인 ViT의 학습과정과 유사하게 CE loss로 학습을 진행한다.</p> <div> <center><img src="../../../assets/img/small03.png" alt="small_supervised" width="50%" height="50%"/></center> </div> <div class="caption"> <center>Self-supervised to Supervised Label Prediction</center> </div> <hr/> <h2 id="experimental-result"><strong>Experimental result</strong></h2> <p>CIFAR나 Tiny-ImageNet 과 같은 small-scale dataset은 32 또는 64 와 같이 낮은 해상도를 가지고 있다. 따라서 실험에 사용되는 patch size와 이에 따른 모델의 구조를 아래와 같이 수정하였다고 한다.</p> <div> <center><img src="../../../assets/img/small04.png" alt="small_exp_set" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Details of ViT encoders used in our proposed training approach</center> </div> <p>실험에는 아래와 같이 7개의 small-scale dataset이 사용되었다.</p> <div> <center><img src="../../../assets/img/small05.png" alt="small_datasets" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Details of datasets</center> </div> <p>실험결과는 아래와 같이 대부분의 실험에 사용된 데이터셋에 대해서 다른 모델보다 높은 성능을 보이는 것을 알 수 있다. Comparison model로는 Dense relative localization task를 활용한 <a href="https://arxiv.org/pdf/2106.03746.pdf">ViT-Drloc</a>과 Shifted Patch Tokenization과 Locality Self-Attention을 활용한 <a href="https://arxiv.org/pdf/2112.13492.pdf">SL-ViT</a>가 있다. 자세한 내용은 원문을 참고하면 좋을 것 같다.</p> <div> <center><img src="../../../assets/img/small06.png" alt="small_exp" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Comparison Performance</center> </div> <p>특히 salient object에 대해서 높은 성능을 보이는데 feature map을 시각화한 아래 figure를 보면 본 연구에서 제안한 모델에서 주변부대비 salient object가 더 잘 활성화 되어 있는 것을 확인할 수 있다.</p> <div> <center><img src="../../../assets/img/small07.png" alt="small_salient" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Comparison with salient objects</center> </div> <p>더 적은 데이터를 활용한 실험결과도 확인할 수 있는데 CIFAR10, CIFAR100의 25%,50%,75%의 데이터로 학습한 결과를 비교해보아도 더 좋은 성능을 보인 것을 확인할 수 있다.</p> <div> <center><img src="../../../assets/img/small08.png" alt="small_part" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Training with part of small dataset</center> </div> <hr/> <h2 id="comments"><strong>Comments</strong></h2> <p>ViT가 처음으로 제안된지 오랜 시간이 지났음에도 불구하고 지금까지도 이에 대한 다양한 연구가 진행되고 있다. 다양한 image recognition task들에서 SOTA model의 자리는 ViT가 차지 한지 오래되었지만, 여전히 해결되지 않고 있는 몇가지 한계점도 있었다. 그 중 하나가 small dataset에 대한 문제였고 앞서 리뷰한 논문뿐만 아니라 여러가지 연구에서 해결하기 위해 다양한 방법을 제안하고 있다. 개인적인 생각으로는 아직도 완전히 해결된 것 처럼보이지는 않지만, 그 단점이 시간이 지나고 연구가 진행됨에 따라서 조금씩 보완되고 있기 때문에 또 어떤 혁신적인 알고리즘이 나오기 전까지는 앞으로도 한동안은 ViT가 대세일 것으로 보인다.</p> <hr/> <h2 id="reference">Reference</h2> <ul> <li><a href="https://arxiv.org/pdf/2104.14294.pdf">Caron, Mathilde, et al. “Emerging properties in self-supervised vision transformers.” Proceedings of the IEEE/CVF international conference on computer vision. 2021.</a></li> <li><a href="https://arxiv.org/pdf/2210.07240.pdf">Gani, Hanan, Muzammal Naseer, and Mohammad Yaqub. “How to Train Vision Transformer on Small-scale Datasets?.” arXiv preprint arXiv:2210.07240 (2022).</a></li> <li><a href="https://arxiv.org/pdf/2112.13492.pdf">Lee, Seung Hoon, Seunghyun Lee, and Byung Cheol Song. “Vision transformer for small-size datasets.” arXiv preprint arXiv:2112.13492 (2021).</a></li> <li><a href="https://arxiv.org/pdf/2106.03746.pdf">Liu, Yahui, et al. “Efficient training of visual transformers with small datasets.” Advances in Neural Information Processing Systems 34 (2021): 23818-23830.</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="review"/><summary type="html"><![CDATA[Proposed method serves as an effective weights initialization to successfully train ViTs from scratch, thus eliminating the need for large-scale pre-training.]]></summary></entry><entry><title type="html">Recurrent Glimpse-based Decoder for Detection with Transformer</title><link href="https://dkdkkim.github.io/blog/2022/rego/" rel="alternate" type="text/html" title="Recurrent Glimpse-based Decoder for Detection with Transformer"/><published>2022-08-09T00:00:00+00:00</published><updated>2022-08-09T00:00:00+00:00</updated><id>https://dkdkkim.github.io/blog/2022/rego</id><content type="html" xml:base="https://dkdkkim.github.io/blog/2022/rego/"><![CDATA[<p>최근 Image recognition의 여러가지 분야에서 Vision Transformer는 많은 성과를 이루어냈다. Object detection 분야에서도 그 트렌드는 다르지 않았는데 그 대표적인예가 <a href="https://arxiv.org/abs/2005.12872">DETR(Detection with Transformer)</a> 라고 할 수 있다. 다만 Vision Transformer의 경우 CNN 에 비해서 상대적으로 많은 데이터로 오랫동안 트레이닝해야 한다는 한계점을 가지고 있다. 이런 단점을 극복하기 위해 여러가지 접근들이 있었는데 이번에 소개할 논문도 그런 접근들 중에 하나이다.</p> <hr/> <h2 id="problem"><strong>Problem</strong></h2> <p>앞에서 말했다시피 DETR의 효과는 여러 연구를 통해서 검증이 되었고 그 연구는 점점 늘어가고 있다. 하지만 DETR은 일정 수준이상의 검출성능을 학습하기까지 아주 오랜시간을 요구한다. MS COCO dataset의 경우를 예로 들자면, <a href="https://arxiv.org/abs/1612.03144">Feature Pyramid Network(FPN)</a>의 경우에는 36 epoch 정도면 되는 성능까지 학습되는데에 <strong>500 epoch</strong> 정도가 소요된다. V100이라는 높은 성능의 GPU 8개를 사용해도 10일 이상이 걸리는 수준이기 때문에 실질적으로 모델을 학습하고 사용하는데에 어려움이 있을 수 밖에 없다.</p> <hr/> <h2 id="approach"><strong>Approach</strong></h2> <p>본 논문에서는 REcurrent Glimpse-based decOder(REGO)라는 모델을 제안하고 있다. 이 모델은 이전의 다른 접근들과는 다른게 RoI에 집중하여 detection을 refinement 할 경우 DETR의 학습의 어려움을 비교적 쉽게 개선한다. 이전의 접근들은 feature 단이나 embedding 부분들을 통해서 문제를 해결하고자하는 방법론들이 대부분이었다.</p> <div> <center><img src="../../../assets/img/rego01.png" alt="rego_approach" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Concept of REcurrent glimpse-based decoder (REGO)</center> </div> <hr/> <h2 id="method"><strong>Method</strong></h2> <p>REGO model의 전체적인 workflow는 DETR의 출력물인 \(O_{box}\)와 feature \(H_{dec}\)을 입력으로 받아 \(N\)개의 step의 반복을 통해서 최종적인 prediction을 하게 되며, 각각의 step은 Glimpse-based Decoder로 구성되어있다.</p> <div> <center><img src="../../../assets/img/rego02.png" alt="rego_method" width="70%" height="70%"/></center> </div> <div class="caption"> <center>The overview of the REGO</center> </div> <h3 id="glimpse-based-decoder">Glimpse-based Decoder</h3> <p>DETR의 output 또는 이전 block의 출력값중 \(O_{box}(i-1)\)을 이용하여 해당영역에서 feature를 추출하게 된다. 이때에 box 값을 그대로 사용하는 것이 아니라 margin을 갖고 조금더 넓은 영역에서 feature를 추출하는데 Diagram에서 Enlarge라고 표시된 부분이다. 이때 enlarge 하는 비율을 나타내는 glimpse scale \(\alpha\) 는 점점 감소하게 되는데 3step일 경우를 예로 들면 \(\alpha=3,2,1\) 이된다.</p> <div> <center><img src="../../../assets/img/rego03.png" alt="rego_method" width="30%" height="30%"/></center> </div> <div class="caption"> <center>Enlarge step of Glimpse-based Decoder</center> </div> <p>이렇게 해당 RoI에서 추출된 feature \(V(i)\)는 query 값이 되고 이전 block으로 부터의 feature \(Hec(i-1)\) 은 Linear Projection으로 dimension 조정 후 key, value 값이 되어 Multi-head cross attention 후 \(H_{g}(i)\) 가 된다. \(H_{g}(i)\) 와 \(Hec(i-1)\)를 concatenation을 통해 하나의 feature로 만들고 MLP를 통해서 새로운 Detected Boxes \(O_{box}(i)\), Classification Output \(O_{cls}(i)\) 그리고 Decoding Output \(H_{dec}(i)\) 를 출력하게 된다.</p> <h3 id="multi-stage-reccurent-processing">Multi-stage Reccurent Processing</h3> <p>이런 glimpes-based Decoder를 $N$번 반복하여 ReGO model 이 완성되며 중간과정에서는 box와 feature 만 사용되지만 마지막 블럭에서는 classification 값까지 최종 예측값으로 사용된다. 본논문에서 일반적으로 step의 수 $N$은 3을 사용하였으며, 이는 실험적으로 선택된 것으로 보인다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rego04-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rego04-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rego04-1400.webp"/> <img src="/assets/img/rego04.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rego05-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rego05-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rego05-1400.webp"/> <img src="/assets/img/rego05.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Hyper-parameter study of the number of stages in REGO and the glimpse scale in REGO </div> <hr/> <h2 id="experimental-result"><strong>Experimental result</strong></h2> <p>MS COCO validation split에서 35, 50 epoch에서의 결과를 아래와 같이 여러가지 existing Object Detection 모델들과 비교했으며, 관심을 가지고 보아야할 부분은 일반적인 DETR과 앞서 Training 문제를 개선한 <a href="https://arxiv.org/abs/2010.04159">Deformable DETR</a> 과의 비교이다.</p> <div> <center><img src="../../../assets/img/rego06.png" alt="rego_method" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Results of different detectors on the MS COCO val split</center> </div> <p>표에서 보면 알 수 있듯이 DETR 은 물론이도 Deformable DETR 보다도 동일 epoch을 training 했을 때 높은 성능을 보임을 알 수 있다. 심지어 동일한 ResNet 50 backbone 기준으로 보았을 때에 REGO로 36 epoch을 training 했을 때 성능(44.8)이 Deformable DETR 의 성능(43.8) 보다 높은 것까지도 확인할 수 있다.</p> <hr/> <h2 id="limitations"><strong>Limitations</strong></h2> <p>본 논문에서 저자도 언급했지만, REGO를 통해서 DETR의 학습 효율은 많이 개선되었지만 여전히 training 하는데에 수 일이 소요되기 때문에 앞으로도 개선의 여지는 남아있다고 볼 수 있다. 그리고 개인적으로는 본 논문에서 성능 검증에 사용된 지표는 validation split의 결과여서 test split에서의 성능의 검증이 필요하고, 50 epoch이상 training 하여 최종적으로 optimized 된 성능의 비교도 확인할 필요가 있다고 생각한다.</p> <hr/> <h2 id="reference">Reference</h2> <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Recurrent_Glimpse-Based_Decoder_for_Detection_With_Transformer_CVPR_2022_paper.html">Chen, Zhe, Jing Zhang, and Dacheng Tao. “Recurrent glimpse-based decoder for detection with transformer.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.</a></li> <li><a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjMmPaL3tz5AhVIQt4KHf0tBMUQFnoECAkQAQ&amp;url=https%3A%2F%2Farxiv.org%2Fabs%2F2005.12872&amp;usg=AOvVaw1fNz92-EEj1d91Nfiv875y">Carion, Nicolas, et al. “End-to-end object detection with transformers.” European conference on computer vision. Springer, Cham, 2020.</a></li> <li><a href="https://arxiv.org/abs/2010.04159">Zhu, Xizhou, et al. “Deformable detr: Deformable transformers for end-to-end object detection.” arXiv preprint arXiv:2010.04159 (2020).</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="review"/><summary type="html"><![CDATA[the Region-of-Interest (RoI) based detection refinement can easily help mitigate the difficulty of training for DETR methods]]></summary></entry><entry><title type="html">Learning Strides In Convolutional Neural Networks</title><link href="https://dkdkkim.github.io/blog/2022/learning-strides/" rel="alternate" type="text/html" title="Learning Strides In Convolutional Neural Networks"/><published>2022-04-26T00:00:00+00:00</published><updated>2022-04-26T00:00:00+00:00</updated><id>https://dkdkkim.github.io/blog/2022/learning-strides</id><content type="html" xml:base="https://dkdkkim.github.io/blog/2022/learning-strides/"><![CDATA[<p>Model의 architecure를 결정하는 여러가지 hyperparameter들이 이전에는 search space등의 방법으로 최선의 조합을 찾았다면, 최근에는 AutoML 방식으로 hyperparameter들도 알고리즘을 통해 자동으로 최적화하는 트렌드이다. 하지만 일반적으로 AutoML은 큰 Resource를 요구하기 때문에 실질적으로 사용하지 못하는 개발자들이 많다. 이번에는 CNN 모델에서 stride를 learnable하게 하는 연구에 대한 논문인 ‘LEARNING STRIDES IN CONVOLUTIONAL NEURAL NETWORKS’을 리뷰하고자 한다. 본 연구는 ICLR 2022에 oral paper로 선정되었으며, stride만을 leanable 하게 하기 때문에 지나치게 많은 resource를 요구하지도 않는다.</p> <hr/> <h2 id="problem"><strong>Problem</strong></h2> <p>CNN base 모델에서 downsampling layer를 디자인 함에 있어서 stride는 flexable하면서 매주 중요한 요소이다. 하지만 최선의 조합을 찾기 위해서는 cross-validation이나 archtecuture search 방식을 사용해야 한다. CNN model의 hyperparameter는 다양하기 때문에 stride까지 hyperparameter로 탐색하게 된다면 search space는 매우 커지게된다. 이는 downsampling layer의 개수가 많아질수록 기하급수적으로 문제가 커진다.</p> <div> <center><img src="../../../assets/img/stride01.png" alt="diffstride" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Hyperparametes of CNN model</center> </div> <hr/> <h2 id="contribution"><strong>Contribution</strong></h2> <p>본 논문에서는 처음으로 제안하는 learnable stride 방법론을 제안하며 이를 ‘DiffStride’라고 명명하였다. 실제로 audio와 image classification task에 적용하였고 실험적으로 그 효과성을 입증하였다.</p> <div> <center><img src="../../../assets/img/stride02.png" alt="diffstride" width="50%" height="60%"/></center> </div> <div class="caption"> <center>Comparison of resiudal block with and without DiffStride</center> </div> <hr/> <h2 id="method"><strong>Method</strong></h2> <h3 id="spectral-pooling">Spectral pooling</h3> <p>DiffStride를 소개하기에 앞서 먼저 Spectral pooling 에 대한 이해가 필요하다. Spectral은 frequency domain에서 power spectrum과 noise간의 특성을 이용해 pooling을 하는 방식을 말한다. 첫번째 단계로 Image \(y\)를 Discrete Fourier Transform(DFT)를 이용하여 frequency domain의 \(\hat y\)으로 변환한다. 그리고 주파수가 낮은 영역을 중심으로 crop한 후 다시 Inverse DFT를 통해서 spatiotemporal signal \(\hat x\) 로 변환한다. 그러면 natural image의 power 기댓값이 통계적으로 저주파 영역에 대부분 밀집되어 있기 때문에 정보량 손실을 최소화하면서 영상의 크기를 감소시킬 수 있다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/stride03-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/stride03-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/stride03-1400.webp"/> <img src="/assets/img/stride03.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/stride04-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/stride04-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/stride04-1400.webp"/> <img src="/assets/img/stride04.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Examples and algorithm of spectral pooling </div> <h3 id="diffstride">DiffStride</h3> <p>DiffStride의 workflow는 아래의 figure와 같다.</p> <div> <center><img src="../../../assets/img/stride05.png" alt="diffstride" width="70%" height="60%"/></center> </div> <div class="caption"> <center>DiffStride forward and backward pass</center> </div> <p>먼저 이미지 또는 feature map \(x\) 는 Fourier Transform \(\it F\) 에 의해 \(y\) 로 변환된다. 그리고 \(y\) 는 Stride parameter \(S\)로 부터 만들어진 \(mask\) 와 Elementwise product를 통해서 저주파 영역 값만 남게 된다. 남은 영역만 crop 한 \(y_{cropped}\) 를 다시 Inverse Fourier Transform 하여 \(\hat x\) 가 되며 이것이 최종 출력물이 된다. 여기까지가 forward flow 였고 backward flow를 보면 빨간색 점선으로 표시되어 있는 순서로 진행된다고 보면 된다. CNN을 학습하기 위한 flow는 기존과 마찬가지로 input image \(x\)까지 흘러가고, stride parameter \(S\)를 학습하기 위한 backward flow는 \(Crop\) 에서는 disconnect 되어 있으므로 element-wise product에서 분기하여 gradient가 계산된다.</p> <div> <center><img src="../../../assets/img/stride06.png" alt="diffstride" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Algorithm of DiffStride</center> </div> <hr/> <h2 id="experimental-result"><strong>Experimental result</strong></h2> <p>논문에서는 image와 audio classification에서 실험을 진행했지만, image classification 에 대한 내용만 정리했다. Backbone은 resnet18로 setting 하였으며 CIFAR 10, CIFAR 100, ImageNet 에 대해서 training 및 evaluation을 진행하였다.</p> <p>Diffstride의 성능은 Vanilla resnet, Spectral pooling이 적용된 resnet과 initial stride 값에 따라 비교 하였다. 아래 표에서 보면 알 수 있듯이 정보 손실량이 적은 spectral pooling의 경우 일반적인 resnet보다 높은 성능을 보였으며, 대부분의 inital stride에 대해서 DiffStride가 적용된 경우가 Spectral pooling이 적용된 경우보다 더 높은 성능을 보이는 것을 알 수 있다.</p> <div> <center><img src="../../../assets/img/stride07.png" alt="diffstride" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Accuracies on CIFAR10 and CIFAR100</center> </div> <div> <center><img src="../../../assets/img/stride08.png" alt="diffstride" width="60%" height="60%"/></center> </div> <div class="caption"> <center> Top-1 and top-5 accuracies (% ± sd over 3 runs) on Imagenet</center> </div> <p>Learnable Stride의 training 경향은 아래 그래프 (a) 에서 처럼 epoch 이 진행됨에 따라 특정 값으로 수렴하는 것을 알 수 있다. 그래프 (b)를 보면 layer의 depth에 따른 Stride의 수렴 값을 알 수 있는데 깊은 level의 layer 일수록 stride의 수렴값이 커지는 것을 알 수 있다. 그리고 그래프 (c) 에서는 width의 stride 값이 height의 stride 값보다 더 큰 값으로 수렴하는 것을 알 수 있다.</p> <hr/> <h2 id="limitations"><strong>Limitations</strong></h2> <p>Spectral pooling의 경우 정보손실량을 줄인다는 점에서 이점이 있지만 그 과정에서 DFT와 Inverse-DFT의 연산과정이 추가되므로 Computational cost 가 증가한다는 한계점이 존재한다. DiffStride는 이에 더해서 Stride parameter를 learnable 하게 하는데에 더 많은 연산량을 요구하기 때문에, Peak Memory 관점에서는 최대 2배 가까운 메모리를 요구할 수도 있다. 이는 저자도 논문에서 언급한 바이다.</p> <div> <center><img src="../../../assets/img/stride09.png" alt="diffstride" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Per-step time and peak memory usage of Spectral Pooling and DiffStride</center> </div> <p>그리고 실험결과를 보면 일부 stride의 initial의 경우에는 DiffStride의 성능이 더 떨어지는 경우가 발생하는데 이는 layer depth가 더 많아지고 initial stride 의 경우의 수가 많아질때에는 intial stride 자체가 새로운 hyperparameter 가 되는 문제가 발생할 수도 있다고 개인적으로 생각한다.</p> <hr/> <h2 id="reference">Reference</h2> <ul> <li><a href="https://arxiv.org/abs/2202.01653">Riad, Rachid, et al. “Learning strides in convolutional neural networks.” arXiv preprint arXiv:2202.01653 (2022).</a></li> <li><a href="https://proceedings.neurips.cc/paper/2015/hash/536a76f94cf7535158f66cfbd4b113b6-Abstract.html">Rippel, Oren, Jasper Snoek, and Ryan P. Adams. “Spectral representations for convolutional neural networks.” Advances in neural information processing systems 28 (2015).</a></li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="review"/><summary type="html"><![CDATA[DiffStride, the first downsampling layer with learnable strides.]]></summary></entry><entry><title type="html">CuPy</title><link href="https://dkdkkim.github.io/blog/2022/cupy/" rel="alternate" type="text/html" title="CuPy"/><published>2022-01-04T00:00:00+00:00</published><updated>2022-01-04T00:00:00+00:00</updated><id>https://dkdkkim.github.io/blog/2022/cupy</id><content type="html" xml:base="https://dkdkkim.github.io/blog/2022/cupy/"><![CDATA[<h2 id="use-of-interpolation"><strong>Use of Interpolation</strong></h2> <p>의료영상의 대부분은 spacing이라는 개념이 존재한다. spacing은 쉽게 말해서 1 pixel이 실제로는 어느정도의 area 또는 volume을 의미하는지를 말한다. 예를들어 어떤 영상의 spacing 이 0.3,0.7 이라고 한다면 1 pixel은 실제로는 0.3x0.7의 공간을 의미한다고 볼 수 있다. spacing은 각각 axis에 따라 다른 값을 가지므로 전처리과정 없이는 왜곡된 상태로 보일 수 밖에 없다. 따라서 원본영상을 실제크기로 변환하기 위해서는 interpolation 을 사용하게 된다.</p> <div> <center><img src="../../../assets/img/cupy01.png" alt="rego_approach" width="70%" height="60%"/></center> </div> <div class="caption"> <center>Example of medical image interpolation</center> </div> <p>Interpolation은 image의 크기나 차원수가 커질수록 연산량이 늘어나고 처리시간이 증가할 수 밖에 없다. 3D image를 다루는 현재 task 특성상 이를 개선할 필요가 있어서 탐색하게 되었고, 기존에는 cpu를 이용한 연산으로 interpolation으로 처리하였으나 처리속도를 개선하기 위해서 gpu를 이용하여 interpolation을 수행하는 cupy를 간단히 소개하고자 한다.</p> <hr/> <h2 id="interpolation"><strong>Interpolation</strong></h2> <h3 id="interpolation-with-scipy">Interpolation with scipy</h3> <p>이미지의 전처리에 많은 사용되는 library로 scipy가 있다. 그래서 기존에 interpolation 할때에는 <code class="language-plaintext highlighter-rouge">scipy.ndimage.interpolation.zoom</code> 를 주로 사용하였다.</p> <div> <center><img src="../../../assets/img/cupy02.png" alt="rego_approach" width="60%" height="60%"/></center> </div> <p>이를 대체하기 위해서 gpu를 이용하여 interpolation을 수행할수 있는 방법이 필요했고 그러려면 기존방식에서 어떤 interpolation을 사용했는지 먼저 확인해야했다.</p> <h3 id="interpolation-methods">Interpolation methods</h3> <p>Interpolation을 하는 방식에는 아래와 같이 여러가지 방식이 있다. 왼쪽으로 갈수록 계산하는 방식이 단순한 만큼 적은 연산량을 요구하는 대신 정보 손실량이 크고, 오른쪽으로 갈수록 연산량이 많이 요구되는 대신 조금더 더 자연스럽게 중간값을 계산한다. 1행의 방법들은 1차원의 값들을 interpolation하는 방식이고 2행의 방법들은 같은 방식으로 2차원에서 interpolation을 하는 방식이다. 3차원도 마찬가지로 확장할 수 있다. 이중에서 기존에 사용했던 방식은 cubic interpolation 이었고, 그중에서도 spline cubic interpolation이라는 것을 확인하였다.</p> <div> <center><img src="../../../assets/img/cupy03.png" alt="rego_approach" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Interpolation methods</center> </div> <h3 id="spline-interpolation">Spline interpolation</h3> <p>Spline interpolation은 Runge’s phenomenon에서 유래한 방식이라고 볼 수 있다. Interpolation에서 연산에 사용되는 차원수가 커질수록 더 정교한 계산을 할수 있다. 그런데 일정 차원수 이상일때 아래 그래프의 양쪽에서 보이는 것 처럼 이상값이 발생하는 것을 확인할 수 있다고 한다.</p> <div> <center><img src="../../../assets/img/cupy04.png" alt="rego_approach" width="60%" height="60%"/></center> </div> <div class="caption"> <center>Runge's phenomenon</center> </div> <p>이런 문제를 해결하기 위해서 제안된 기법이 spline interpolation 이다. 제안된 방법의 주요 아이디어는 함수값 $f$ 만 일치시키는 것이 아니라 미분값인 $f’$ 까지도 일치시켜 주자는 것이다. 이렇게 함으로서 더 좋은 결과를 얻을수 있으며 Runge’s phenomenon을 방지 할 수 있다. $(x_i,y_i)$ 와 $(x_{i+1},y_{i+1})$ 사이의 cubic spline function $S_{i}(x)$ 는 아래와 같이 계산할 수 있다.</p> <p>\(S_i(x) = a_i(x-x_i)^3 + b_i(x-x_i)^2 + c_i(x-x_i) + d_i\\ S_i(x) = y_i\\ S_i(x_{i+1}) =y_{i+1}\) \(S_{i-1}'(x_i) = S_i'(x_i)\\ S_{i-1}''(x_i) = S_i''(x_i)\) \(S_{i-1}'(x_{i+1}) = S_i'(x_{i+1})\\ S_{i-1}''(x_{i+1}) = S_i''(x_{i+1})\)</p> <hr/> <h2 id="cupy"><strong>Cupy</strong></h2> <p>Cupy는 GPU 계산을 Numpy와 유사한 방법으로 수행할 수 있도록 개발된 Library 이다. 초기버전이었던 PyCUDA는 CUDA 문법이 사용되기 때문에 개발자들의 진입장벽이 높았으나 Cupy의 경우에는 Numpy와 거의 동일한 방식으로 사용할 수 있기 때문에 사용하는데에 어려움이 없다.</p> <h3 id="installation"><strong>Installation</strong></h3> <p>Cupy는 NVIDIA의 GPU와 CUDA를 기반으로 하기 때문에 CUDA 환경만 세팅되어 있으면 아래와 같이 간단히 설치할 수 있다.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install cupy-cuda11x
</code></pre></div></div> <h3 id="implementation"><strong>Implementation</strong></h3> <p>아래와 같이 Cupy를 활용하여 interpolation 방식으로 zoom 하는 코드를 구성할 수 있다.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">resize_us_cupy</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">spacing</span><span class="p">,</span> <span class="n">new_spacing</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="s">"""
    image: input image
    spacing: spacing of input image
    new_spacing: spacing of zoomed image
    order: order of interpolation
    """</span>
    <span class="kn">import</span> <span class="n">cupy</span>
    <span class="kn">import</span> <span class="n">cupyx.scipy.ndimage</span>
    <span class="n">mempool</span> <span class="o">=</span> <span class="n">cupy</span><span class="p">.</span><span class="nf">get_default_memory_pool</span><span class="p">()</span>
    <span class="n">pinned_mempool</span> <span class="o">=</span> <span class="n">cupy</span><span class="p">.</span><span class="nf">get_default_pinned_memory_pool</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">cupy</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nc">Device</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">cupy</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="n">resize_factor</span> <span class="o">=</span> <span class="n">spacing</span> <span class="o">/</span> <span class="n">new_spacing</span>
        <span class="n">new_real_shape</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span> <span class="o">*</span> <span class="n">resize_factor</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">new_real_shape</span><span class="p">)</span>
        <span class="n">real_resize_factor</span> <span class="o">=</span> <span class="n">new_shape</span> <span class="o">/</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">us</span> <span class="o">=</span> <span class="n">cupyx</span><span class="p">.</span><span class="n">scipy</span><span class="p">.</span><span class="n">ndimage</span><span class="p">.</span><span class="nf">zoom</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">real_resize_factor</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">)</span>
        <span class="n">us_cpu</span> <span class="o">=</span> <span class="n">us</span><span class="p">.</span><span class="nf">get</span><span class="p">()</span>
        <span class="n">mempool</span><span class="p">.</span><span class="nf">free_all_blocks</span><span class="p">()</span>
        <span class="n">pinned_mempool</span><span class="p">.</span><span class="nf">free_all_blocks</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">us_cpu</span>
</pre></td></tr></tbody></table></code></pre></figure> <hr/> <h2 id="comparation"><strong>Comparation</strong></h2> <p>Cupy 공식 홈페이지에서 아래와 같이 Cupy와 Numpy의 연산속도를 비교하였다. 대부분의 operation에서 높은 효율의 향상을 확인하였다. array의 크기가 클수록 효과성은 더 크고 최대 200배가 넘는 속도향상을 보였다.</p> <div> <center><img src="../../../assets/img/cupy05.png" alt="rego_approach" width="70%" height="60%"/></center> </div> <div class="caption"> <center>Cupy speedup over Numpy</center> </div> <p>실제로 앞에서 설명한 코드로 330x608x865 크기의 array를 처리했을때 기존에 Numpy를 활용하였을 때에 15.3s 이 걸렸던 zoom을 4.4s만에 처리할 수 있었다.</p> <h2 id="reference">Reference</h2> <ul> <li><a href="https://cupy.dev/">CuPy: NumPy &amp; SciPy for GPU</a></li> <li><a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwjx9aSj1fv5AhWJAd4KHS63AcIQFnoECAwQAQ&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FSpline_interpolation&amp;usg=AOvVaw0RfZsj2O_0PBxgSxwq7D4B">Spline interpolation - Wikipedia</a></li> <li><a href="https://qt3b1s62da6s.tistory.com/415">Cubic Spline interplation</a></li> </ul>]]></content><author><name></name></author><category term="python-library"/><category term="skills"/><summary type="html"><![CDATA[NumPy & SciPy for GPU]]></summary></entry></feed>