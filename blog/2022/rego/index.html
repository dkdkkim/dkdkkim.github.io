<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Recurrent Glimpse-based Decoder for Detection with Transformer | Daekyung Kim</title> <meta name="author" content="Daekyung Kim"/> <meta name="description" content="the Region-of-Interest (RoI) based detection refinement can easily help mitigate the difficulty of training for DETR methods"/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>📟</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://dkdkkim.github.io/blog/2022/rego/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://dkdkkim.github.io/"><span class="font-weight-bold">Daekyung</span> Kim</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">projects</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Recurrent Glimpse-based Decoder for Detection with Transformer</h1> <p class="post-meta">August 9, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/review"> <i class="fas fa-hashtag fa-sm"></i> review</a>     ·   <a href="/blog/category/paper-review"> <i class="fas fa-tag fa-sm"></i> paper-review</a>   </p> </header> <article class="post-content"> <p>최근 Image recognition의 여러가지 분야에서 Vision Transformer는 많은 성과를 이루어냈다. Object detection 분야에서도 그 트렌드는 다르지 않았는데 그 대표적인예가 <a href="https://arxiv.org/abs/2005.12872" target="_blank" rel="noopener noreferrer">DETR(Detection with Transformer)</a> 라고 할 수 있다. 다만 Vision Transformer의 경우 CNN 에 비해서 상대적으로 많은 데이터로 오랫동안 트레이닝해야 한다는 한계점을 가지고 있다. 이런 단점을 극복하기 위해 여러가지 접근들이 있었는데 이번에 소개할 논문도 그런 접근들 중에 하나이다.</p> <hr> <h2 id="problem"><strong>Problem</strong></h2> <p>앞에서 말했다시피 DETR의 효과는 여러 연구를 통해서 검증이 되었고 그 연구는 점점 늘어가고 있다. 하지만 DETR은 일정 수준이상의 검출성능을 학습하기까지 아주 오랜시간을 요구한다. MS COCO dataset의 경우를 예로 들자면, <a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener noreferrer">Feature Pyramid Network(FPN)</a>의 경우에는 36 epoch 정도면 되는 성능까지 학습되는데에 <strong>500 epoch</strong> 정도가 소요된다. V100이라는 높은 성능의 GPU 8개를 사용해도 10일 이상이 걸리는 수준이기 때문에 실질적으로 모델을 학습하고 사용하는데에 어려움이 있을 수 밖에 없다.</p> <hr> <h2 id="approach"><strong>Approach</strong></h2> <p>본 논문에서는 REcurrent Glimpse-based decOder(REGO)라는 모델을 제안하고 있다. 이 모델은 이전의 다른 접근들과는 다른게 RoI에 집중하여 detection을 refinement 할 경우 DETR의 학습의 어려움을 비교적 쉽게 개선한다. 이전의 접근들은 feature 단이나 embedding 부분들을 통해서 문제를 해결하고자하는 방법론들이 대부분이었다.</p> <div> <center><img src="../../../assets/img/rego01.png" alt="rego_approach" width="60%" height="60%"></center> </div> <div class="caption"> <center>Concept of REcurrent glimpse-based decoder (REGO)</center> </div> <hr> <h2 id="method"><strong>Method</strong></h2> <p>REGO model의 전체적인 workflow는 DETR의 출력물인 \(O_{box}\)와 feature \(H_{dec}\)을 입력으로 받아 \(N\)개의 step의 반복을 통해서 최종적인 prediction을 하게 되며, 각각의 step은 Glimpse-based Decoder로 구성되어있다.</p> <div> <center><img src="../../../assets/img/rego02.png" alt="rego_method" width="70%" height="70%"></center> </div> <div class="caption"> <center>The overview of the REGO</center> </div> <h3 id="glimpse-based-decoder">Glimpse-based Decoder</h3> <p>DETR의 output 또는 이전 block의 출력값중 \(O_{box}(i-1)\)을 이용하여 해당영역에서 feature를 추출하게 된다. 이때에 box 값을 그대로 사용하는 것이 아니라 margin을 갖고 조금더 넓은 영역에서 feature를 추출하는데 Diagram에서 Enlarge라고 표시된 부분이다. 이때 enlarge 하는 비율을 나타내는 glimpse scale \(\alpha\) 는 점점 감소하게 되는데 3step일 경우를 예로 들면 \(\alpha=3,2,1\) 이된다.</p> <div> <center><img src="../../../assets/img/rego03.png" alt="rego_method" width="30%" height="30%"></center> </div> <div class="caption"> <center>Enlarge step of Glimpse-based Decoder</center> </div> <p>이렇게 해당 RoI에서 추출된 feature \(V(i)\)는 query 값이 되고 이전 block으로 부터의 feature \(Hec(i-1)\) 은 Linear Projection으로 dimension 조정 후 key, value 값이 되어 Multi-head cross attention 후 \(H_{g}(i)\) 가 된다. \(H_{g}(i)\) 와 \(Hec(i-1)\)를 concatenation을 통해 하나의 feature로 만들고 MLP를 통해서 새로운 Detected Boxes \(O_{box}(i)\), Classification Output \(O_{cls}(i)\) 그리고 Decoding Output \(H_{dec}(i)\) 를 출력하게 된다.</p> <h3 id="multi-stage-reccurent-processing">Multi-stage Reccurent Processing</h3> <p>이런 glimpes-based Decoder를 $N$번 반복하여 ReGO model 이 완성되며 중간과정에서는 box와 feature 만 사용되지만 마지막 블럭에서는 classification 값까지 최종 예측값으로 사용된다. 본논문에서 일반적으로 step의 수 $N$은 3을 사용하였으며, 이는 실험적으로 선택된 것으로 보인다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rego04-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rego04-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rego04-1400.webp"></source> <img src="/assets/img/rego04.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rego05-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rego05-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rego05-1400.webp"></source> <img src="/assets/img/rego05.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Hyper-parameter study of the number of stages in REGO and the glimpse scale in REGO </div> <hr> <h2 id="experimental-result"><strong>Experimental result</strong></h2> <p>MS COCO validation split에서 35, 50 epoch에서의 결과를 아래와 같이 여러가지 existing Object Detection 모델들과 비교했으며, 관심을 가지고 보아야할 부분은 일반적인 DETR과 앞서 Training 문제를 개선한 <a href="https://arxiv.org/abs/2010.04159" target="_blank" rel="noopener noreferrer">Deformable DETR</a> 과의 비교이다.</p> <div> <center><img src="../../../assets/img/rego06.png" alt="rego_method" width="60%" height="60%"></center> </div> <div class="caption"> <center>Results of different detectors on the MS COCO val split</center> </div> <p>표에서 보면 알 수 있듯이 DETR 은 물론이도 Deformable DETR 보다도 동일 epoch을 training 했을 때 높은 성능을 보임을 알 수 있다. 심지어 동일한 ResNet 50 backbone 기준으로 보았을 때에 REGO로 36 epoch을 training 했을 때 성능(44.8)이 Deformable DETR 의 성능(43.8) 보다 높은 것까지도 확인할 수 있다.</p> <hr> <h2 id="limitations"><strong>Limitations</strong></h2> <p>본 논문에서 저자도 언급했지만, REGO를 통해서 DETR의 학습 효율은 많이 개선되었지만 여전히 training 하는데에 수 일이 소요되기 때문에 앞으로도 개선의 여지는 남아있다고 볼 수 있다. 그리고 개인적으로는 본 논문에서 성능 검증에 사용된 지표는 validation split의 결과여서 test split에서의 성능의 검증이 필요하고, 50 epoch이상 training 하여 최종적으로 optimized 된 성능의 비교도 확인할 필요가 있다고 생각한다.</p> <hr> <h2 id="reference">Reference</h2> <ul> <li><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Recurrent_Glimpse-Based_Decoder_for_Detection_With_Transformer_CVPR_2022_paper.html" target="_blank" rel="noopener noreferrer">Chen, Zhe, Jing Zhang, and Dacheng Tao. “Recurrent glimpse-based decoder for detection with transformer.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.</a></li> <li><a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjMmPaL3tz5AhVIQt4KHf0tBMUQFnoECAkQAQ&amp;url=https%3A%2F%2Farxiv.org%2Fabs%2F2005.12872&amp;usg=AOvVaw1fNz92-EEj1d91Nfiv875y" target="_blank" rel="noopener noreferrer">Carion, Nicolas, et al. “End-to-end object detection with transformers.” European conference on computer vision. Springer, Cham, 2020.</a></li> <li><a href="https://arxiv.org/abs/2010.04159" target="_blank" rel="noopener noreferrer">Zhu, Xizhou, et al. “Deformable detr: Deformable transformers for end-to-end object detection.” arXiv preprint arXiv:2010.04159 (2020).</a></li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Daekyung Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. Last updated: April 15, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>