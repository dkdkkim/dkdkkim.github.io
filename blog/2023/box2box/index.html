<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Box2Mask, Box-supervised Instance Segmentation via Level-set Evolution | Daekyung Kim</title> <meta name="author" content="Daekyung Kim"/> <meta name="description" content="box-supervised instance segmentation takes advantage of the simple box annotation rather than the pixel-wise mask labels, which has recently attracted a lot of research attentions."/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>📟</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://dkdkkim.github.io/blog/2023/box2box/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://dkdkkim.github.io/"><span class="font-weight-bold">Daekyung</span> Kim</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">projects</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Box2Mask, Box-supervised Instance Segmentation via Level-set Evolution</h1> <p class="post-meta">January 25, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/review"> <i class="fas fa-hashtag fa-sm"></i> review</a>     ·   <a href="/blog/category/paper-review"> <i class="fas fa-tag fa-sm"></i> paper-review</a>   </p> </header> <article class="post-content"> <p>Segmentation 기술은 지속적으로 발전되어 현재의 SOTA 모델의 성능은 상당히 높은 수준에 이르렀으며, 실제 필드에서도 여러가지 적용사례를 보여주고 있다. 하지만 Segmentation task의 한계는 Labeling cost 라고 할 수 있다. Segmentation을 하기 위한 Ground truth mask 는 pixel 단위로 annotation 을 해야하기 때문에 Classification 이나 detection 등에 비해서 많은 Human resource 를 필요로 한다. 이런 한계를 극복하고자 제안한 접근중에 하나가 Box-supervised Instance Segmentation 이다. 직관적으로 알 수 있듯이 Detection 에 사용되는 Bounding Box를 활용해 Instance Segmentation의 기능을 하도록 하는 것으로 일종의 Weakly supervised learning 이라고 할 수 있다. 다른 Task 에 비해서 비교적 많은 연구가 이루어 지지는 않았지만, 흥미로운 주제이기에 현재 기준으로 SOTA model을 달성한 연구에 대해서 알아보고자 한다.</p> <hr> <h2 id="problem"><strong>Problem</strong></h2> <p>이미 앞서 설명했지만, 대부분의 instance segmentation 접근법들은 supervised learning 방식으로 이루어지고 있으며, 이에 따라서 pixel level로 mask annotation을 하는데에 상당히 큰 cost를 필요로 한다. box-supervised instance segmentation은 상대적으로 단순한 box annotation 만 있으면 되기 때문에 조금씩 연구가 이루어 지고 있다. Paper with code 에서 box-supervised instance segmentation 의 Ranking을 확인해보면 아래와 같이 2019년 이후로 지속적으로 연구가 이루어 지고 있음을 알 수 있다.</p> <div> <center><img src="../../../assets/img/box01.png" alt="box_rank" width="60%" height="60%"></center> </div> <div class="caption"> <center>Box-supervised Instance Segmentation on COCO test-dev</center> </div> <hr> <h2 id="approach"><strong>Approach</strong></h2> <p>본 논문에서는 <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/cd3afef9b8b89558cd56638c3631868a-Paper.pdf" target="_blank" rel="noopener noreferrer">SOLOV2</a> 와 Level Set Segmentation을 활용한 Box-instance segmentation을 제안한다. SOLOV2는 full instance mask annotations를 이용하여 Segmentation 하는 <a href="https://arxiv.org/pdf/1912.04488.pdf" target="_blank" rel="noopener noreferrer">SOLO</a>의 개선 모델이다. Level Set Segmentation은 전통적인 Computer vision 에서 이전부터 사용되던 방법중의 하나이다. 전체적인 Architecture는 아래와 같다.</p> <div> <center><img src="../../../assets/img/box02.png" alt="box_arch" width="60%" height="60%"></center> </div> <div class="caption"> <center>Architecture of BOX2SEG</center> </div> <hr> <h2 id="method"><strong>Method</strong></h2> <p>본 논문에서 제안하는 방법은 크게 SOLOV2와 Level Set Evolution 으로 구성되어 있는 것을 알 수 있다. 따라서 이 두가지 방법에 대한 이해가 선행되어야 한다.</p> <h3 id="solo">SOLO</h3> <p>SOLOV2 는 SOLO 의 개선된 버전이기 때문에 SOLO를 먼저 설명하려한다. SOLO 는 기존에 접근과는 다른 방식으로 Instance segmentation을 수행한다. 먼저 Image 를 \(S \times S\) 로 분할하여 접근하며 FPN 의 모든 Feature map 에 대해서 Category branch와 Mask branch가 적용된다. 아래 그림이 SOLO에서 제안하는 방식이다.</p> <div> <center><img src="../../../assets/img/box03.png" alt="small_solo" width="60%" height="60%"></center> </div> <div class="caption"> <center>Concept of SOLO</center> </div> <p>Category Branch 의 output 은 \(S \times S \times C\) 의 shape를 가지고 있으며 \(S \times S\) 그리드의 Class를 나타낸다.</p> <p>Mask Branch 의 output 은 \(H \times W \times S^2\) 으로 \(S^2\) 개의 그리드에 해당하는 segmentation mask 이다. Mask Branch 에서는 각각의 mask 가 그리드에 match 되도록 하기 위하여 x, y 좌표를 normalize 한 값을 concat 하는 CoordConv 를 사용하였다.</p> <p>결과적으로 두 브랜치의 output을 활용하면 각각의 그리드에 해당하는 Segmentation mask와 Category 를 알 수 있게 된다.</p> <h3 id="solo-v2">SOLO V2</h3> <p>하지만 SOLO 에 대한 방법을 보다보면 걱정이 되는 부분이 생길 수 밖에 없을 것이다. 바로 SOLO의 방법은 높은 Computational Cost를 필요로 한다는 점이다. 이런 단점을 극복한 방법이 SOLO V2 이다. SOLO 에서 사용되었던 Category branch와 Mask branch 대신에 kernel branch와 feature branch 가 새롭게 제안되었다.</p> <div> <center><img src="../../../assets/img/box04.png" alt="small_solo" width="60%" height="60%"></center> </div> <div class="caption"> <center>Concept of SOLO V2</center> </div> <p>feauter branch 의 output \(F\) 는 \(H \times W \times E\) 의 shape를 가진다. 이때 \(E\) 는 N of channel로 보면 된다.</p> <p>kernel branch 가 key idea 라고 할 수 있는데, 여기서는 \(F\) 에 Convolution 연산을 할 filter 의 weight를 학습하는 역할을 하게 된다. Output \(G\) 는 \(S \times S \times D\) 를 이루게 되는데 이는 \(S^2\) 개의 filter weight를 나타내며 각각의 그리드에 해당한다. 예를들어 \(3 \times 3\) 의 conv filter를 적용하려고 한다면 \(D = E \times (3 \times 3)\) 이된다.</p> <p>결과적으로 \(F\) 와 \(G\) 가 convolution 연산을 하게 되면 SOLO 와 마찬가지로 \(H \time W \times S^2\) 의 segmentation map이 출력된다. 앞서 설명한 방법으로 SOLO V2 는 SOLO 보다 효율적이면서 더 높은 성능을 보였다.</p> <h3 id="level-set-segmentation">Level-set segmentation</h3> <p>Level-set segmentation은 level-set function을 활용하여 image 를 segmentation 하는 방식을 말한다. Level-set function을 \(\phi (x)\) 라고 가정하면 \(\{x \mid \phi (x) = c \}\)를 활용하여 아래와 같이 segmentation을 수행할 수 있다.</p> <div> <center><img src="../../../assets/img/box05.png" alt="small_solo" width="60%" height="60%"></center> </div> <div class="caption"> <center>Level set segmentation(wikipedia)</center> </div> <p>Level-set segmentaton의 좋은 결과를 내기 위해서는 최적의 level-set function을 찾아야 하는데 이때 사용하는 방식이 Energy function을 활용하는 방식이다. Level-set function 에 대한 Energy function을 정의하여 Energy가 최소화 하도록 Level-set function을 반복하여 업데이트 하여 최적화 하는 방식이다.</p> <div> <center><img src="../../../assets/img/box06.png" alt="small_solo" width="60%" height="60%"></center> </div> <div class="caption"> <center></center> </div> <p>위와 같은 식으로 업데이트 되는데 이때 initial level-set function과 Energy function을 어떤 방식으로 정의하느냐에 따라서 그 결과는 달라진다. 이 Energy function을 정의 하는 방법중의 하나가 <a href="">Active contours without edges</a> 이다.</p> <div> <center><img src="../../../assets/img/box07.png" alt="small_solo" width="100%" height="60%"></center> </div> <div class="caption"> <center></center> </div> <p>위의 식이 <a href="https://www.lpi.tel.uva.es/muitic/pim/docus/ActiveContoursWithoutEdges.pdf" target="_blank" rel="noopener noreferrer">Active contours without edges</a>에서 제안한 Energy function 이다. 개념적으로 간단히 설명하자면 4개의 term 으로 이루어져 있고 각각 contour의 length, \(\phi \ge 0\) area의 넗이, inside area, outside area를 의미한다. \(c1, c2\) 는 각각 inside 와 outside의 intensity average를 이다.</p> <h3 id="box2seg">BOX2SEG</h3> <p>본 연구에서 제안한 BOX2SEG 에서는 SOLO V2를 통해서 나온 output 을 initial level-set function \({\phi}_0\) 로 가정하고 SOLO V2의 feature branch의 output을 Conv layer를 통과시킨 \(I_{feat}\), 원본 이미지에서 추출된 feature map \(I_{img}\) 를 활용하여 Level-set evolution을 수행한다.</p> <div> <center><img src="../../../assets/img/box08.png" alt="small_solo" width="60%" height="60%"></center> </div> <div class="caption"> <center>Level set evolution</center> </div> <p>Level set evolution 에서는 아래와 같은 식으로 Energy function을 정의하는데 Active contours without edges 와 유사한 방식임을 알 수 있다.</p> <div> <center><img src="../../../assets/img/box09.png" alt="small_solo" width="60%" height="60%"></center> </div> <div class="caption"> <center></center> </div> <p>input feature map \(I_f\), \(I_u\) 를 weighted summation 하여 아래와 같이 최종적인 energy function이 정의되고, 앞서 설명한 방식대로 level-set function을 최적화 한다.</p> <div> <center><img src="../../../assets/img/box10.png" alt="small_solo" width="60%" height="60%"></center> </div> <div class="caption"> <center></center> </div> <p>모델의 학습에 쓰이는 loss function은 \(wL_{cate} + L_{inst}\) 로 category classification loss 와 instance segmentation loss로 이루어져 있으며 \(L_{cate}\) 는 일반적인 cross entropy loss를 사용하고 \(L_(inst)\) 는 앞서 설명한 energy function \(F(x)\) 와 \(F(\phi_0)_{box}\) 로 이루어져 있는데 \(F(\phi_0)_{box}\) 는 아래 식에서 알 수 있듯이 predicted mask 와 box label을 각 axis 에 projection 했을때 차이를 나타낸다.</p> <div> <center><img src="../../../assets/img/box11.png" alt="small_solo" width="60%" height="60%"></center> </div> <div class="caption"> <center></center> </div> <p>SOLO와 level-set function 까지 함께 설명하자니 내용이 복잡해졌는데, 여기까지가 BOX2SEG의 Method 에 대한 설명이다.</p> <hr> <h2 id="experimental-result"><strong>Experimental result</strong></h2> <p>BOX2SEG의 성능을 검증하기 위하여 Pascal VOC, COCO, iSAID, LiTS(Liver tumor), ICDAR2019 ReCTS 의 다양한 데이터 셋에서 실험을 진행하였다. 아래는 COCO dataset에서의 결과인데, box-supervised 에서 가장 좋은 성능을 보이는 것을 알 수 있다. 게다가 mask-supervised 와 비교했을 때에도 대부분의 모델보다 좋은 성능을 보이는 것을 알 수 있다.</p> <div> <center><img src="../../../assets/img/box12.png" alt="small_solo" width="60%" height="60%"></center> </div> <div class="caption"> <center>Experimental result on COCO test-dev</center> </div> <p>아래 정성적인 결과를 보면 general image 뿐만 아니라 medical image, scene text 등에도 꽤나 효과적인 것을 확인할 수 있다.</p> <div> <center><img src="../../../assets/img/box13.png" alt="small_solo" width="60%" height="60%"></center> </div> <div class="caption"> <center>Qualitative result of BOX2SEG</center> </div> <hr> <h2 id="comments"><strong>Comments</strong></h2> <p>최근 효과적인 Large model들에 대한 연구들이 좋은 성과를 내기 시작하고 있다. 성공한 연구들을 보면 NLP나 Image classification 처럼 충분한 데이터를 갖춘 분야들임을 알 수 있다. Segmentation이라는 task는 이런 데이터 관점에서 매우 불리한 Task이고 다른 task 처럼 방대한 데이터셋은 언제 구축될 수 있을 지 모른다. 이런 관점에서 box-supervised segmentation과 같은 접근은 적어도 한동안은 연구되어야 할 분야라는 생각이 든다.</p> <hr> <h2 id="reference">Reference</h2> <ul> <li><a href="https://arxiv.org/pdf/2212.01579v1.pdf" target="_blank" rel="noopener noreferrer">Li, Wentong, et al. “Box2Mask: Box-supervised Instance Segmentation via Level-set Evolution.” arXiv preprint arXiv:2212.01579 (2022).</a></li> <li><a href="https://arxiv.org/abs/1912.04488" target="_blank" rel="noopener noreferrer">Wang, Xinlong, et al. “Solo: Segmenting objects by locations.” Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVIII 16. Springer International Publishing, 2020.</a></li> <li><a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/cd3afef9b8b89558cd56638c3631868a-Paper.pdf" target="_blank" rel="noopener noreferrer">Wang, Xinlong, et al. “Solov2: Dynamic and fast instance segmentation.” Advances in Neural information processing systems 33 (2020): 17721-17732.</a></li> <li><a href="https://www.lpi.tel.uva.es/muitic/pim/docus/ActiveContoursWithoutEdges.pdf" target="_blank" rel="noopener noreferrer">Chan, Tony F., and Luminita A. Vese. “Active Contours Without Edges.” IEEE TRANSACTIONS ON IMAGE PROCESSING 10.2 (2001).</a></li> <li><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Tian_BoxInst_High-Performance_Instance_Segmentation_With_Box_Annotations_CVPR_2021_paper.pdf" target="_blank" rel="noopener noreferrer">Tian, Zhi, et al. “Boxinst: High-performance instance segmentation with box annotations.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.</a></li> <li><a href="https://en.wikipedia.org/wiki/Level-set_method" target="_blank" rel="noopener noreferrer">Level set function, wikipedia</a></li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Daekyung Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. Last updated: April 15, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>